{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import timeit\n",
    "import datetime\n",
    "import cupy as cu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf as cd\n",
    "import io\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "init_time = timeit.default_timer()\n",
    "init_time_2 = timeit.default_timer()\n",
    "\n",
    "# SECTION 1: DATA INPUTS\n",
    "sites = cd.read_csv(\"site_daily_small.csv\", delimiter=\",\")\n",
    "hr_sites = cd.read_csv(\"bin_hourly.csv\", delimiter=\",\")\n",
    "npv_values = cd.read_csv(\"npv.csv\", delimiter=\",\")\n",
    "lte_params = cd.read_csv('lte_assumptions.csv', delimiter=',')\n",
    "\n",
    "# sites = sites.sample(frac=.3, random_state=1)\n",
    "# hr_sites = hr_sites.sample(frac=.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/miniconda3/envs/rapids_SSA/lib/python3.7/site-packages/cudf/core/frame.py:2156: UserWarning: When using a sequence of booleans for `ascending`, `na_position` flag is not yet supported and defaults to treating nulls as greater than all numbers\n",
      "  \"When using a sequence of booleans for `ascending`, \"\n"
     ]
    }
   ],
   "source": [
    "# candidate dropping criteria\n",
    "npv_threshold = 0\n",
    "bin_prox = 17\n",
    "sinr_deg = 10\n",
    "\n",
    "sites = sites.dropna()\n",
    "hr_sites = hr_sites.dropna()\n",
    "first_sites = sites\n",
    "\n",
    "rb_bins = sites.merge(hr_sites, on=['GridName'])\n",
    "rb_bins['bin_req_hr_mbps'] = (rb_bins['Hour_GBs'] * 8 * (2 ** 10)) / 3600\n",
    "sum_req = rb_bins.sort_values(by=['GridName', 'bin_req_hr_mbps'], ascending=[True, False])\n",
    "bin_req = rb_bins.groupby('GridName')['bin_req_hr_mbps'].max().reset_index()\n",
    "\n",
    "day_usage = hr_sites.groupby(\"GridName\")[\"Hour_GBs\"].sum().reset_index()\n",
    "sites = sites.merge(day_usage, on=[\"GridName\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NPV DATA\n",
    "npv_values.columns = [\"non\", \"pole_du\", \"pole_u\", \"pole_s\"]\n",
    "disc_rt = 0.15\n",
    "\n",
    "# LTE INPUTS\n",
    "rx_sensitivity_db = -89.4\n",
    "\n",
    "# SECTION 2: FUNCTIONS\n",
    "\n",
    "\n",
    "def asset(asset_in):\n",
    "    asset_type = 0\n",
    "    if asset_in == \"Pole\":\n",
    "        asset_type = 1\n",
    "    return asset_type\n",
    "\n",
    "\n",
    "def morph(morph_in):\n",
    "    morph_type = 0\n",
    "    if morph_in == \"Dense Urban\":\n",
    "        morph_type = 11\n",
    "    elif morph_in == \"Urban\":\n",
    "        morph_type = 13\n",
    "    elif morph_in == \"Suburban\":\n",
    "        morph_type = 17\n",
    "    return morph_type\n",
    "\n",
    "\n",
    "morph_pairs = {\n",
    "    11: 1,\n",
    "    13: 2,\n",
    "    17: 3,\n",
    "    19: 4,\n",
    "    55: 9,\n",
    "    65: 10,\n",
    "    85: 11,\n",
    "    95: 12,\n",
    "    33: 13,\n",
    "    39: 14,\n",
    "    51: 15,\n",
    "    57: 16,\n",
    "    77: 5,\n",
    "    91: 6,\n",
    "    119: 7,\n",
    "    133: 8,\n",
    "}\n",
    "\n",
    "\n",
    "def asset_morph(asset_num, morph_num):\n",
    "    a_m = asset_num * morph_num\n",
    "    return morph_pairs[a_m]\n",
    "\n",
    "\n",
    "def morph_array_pole(code):\n",
    "    ma_pole = ''\n",
    "    if code == 1:\n",
    "        ma_pole = pole_du\n",
    "    elif code == 2:\n",
    "        ma_pole = pole_u\n",
    "    elif code == 3:\n",
    "        ma_pole = pole_s\n",
    "    return ma_pole\n",
    "\n",
    "\n",
    "def fin_arrays_pole(code):\n",
    "    cpx_pole = np.empty([11, 1])\n",
    "    opx_pole = np.empty([11, 1])\n",
    "    growth_pole = np.empty([11, 1])\n",
    "    mvno_pole = np.empty([11, 1])\n",
    "    npv_array_pole = np.zeros([len(npv_values), 1])\n",
    "    if code == 1:\n",
    "        npv_array_pole = npv_values.loc[:, \"pole_du\"]\n",
    "    elif code == 2:\n",
    "        npv_array_pole = npv_values.loc[:, \"pole_u\"]\n",
    "    elif code == 3:\n",
    "        npv_array_pole = npv_values.loc[:, \"pole_s\"]\n",
    "    elif code == 4:\n",
    "        npv_array_pole = npv_values.loc[:, \"\"]\n",
    "    for m in range(11):\n",
    "        cpx_pole[m] = float(npv_array_pole[m + 1])\n",
    "    for m in range(11):\n",
    "        opx_pole[m] = float(npv_array_pole[m + 12])\n",
    "    for m in range(11):\n",
    "        growth_pole[m] = float(npv_array_pole[m + 23])\n",
    "    for m in range(11):\n",
    "        mvno_pole[m] = float(npv_array_pole[m + 34])\n",
    "    end_array = np.hstack((cpx_pole, opx_pole, growth_pole, mvno_pole))\n",
    "    return end_array\n",
    "\n",
    "\n",
    "def bld_npv21(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[1:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 1\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv22(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[2:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 2\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv23(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[3:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 3\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv24(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[4:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 4\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv25(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[5:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 5\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv26(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[6:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 6\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv27(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[7:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 7\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv28(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[8:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 8\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv29(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[9:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 9\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def bld_npv30(gbs, code):\n",
    "    array = morph_array_pole(code)\n",
    "    array = array[10:11, :]\n",
    "    value = (gbs * 365 * array[0][3] * array[0][2] - array[0][1] - array[0][0]) / (\n",
    "            1.15 ** 10\n",
    "    )\n",
    "    if value > 0:\n",
    "        year = 1\n",
    "    elif value < 0:\n",
    "        year = 12\n",
    "    else:\n",
    "        year = 11\n",
    "    return year\n",
    "\n",
    "\n",
    "def loop_npv21(gbs, code):\n",
    "    value = np.empty([10, 1])\n",
    "    year = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(10):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv22(gbs, code):\n",
    "    value = np.empty([9, 1])\n",
    "    year = np.array([2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(9):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv23(gbs, code):\n",
    "    value = np.empty([8, 1])\n",
    "    year = np.array([3, 4, 5, 6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(8):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv24(gbs, code):\n",
    "    value = np.empty([7, 1])\n",
    "    year = np.array([4, 5, 6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(7):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv25(gbs, code):\n",
    "    value = np.empty([6, 1])\n",
    "    year = np.array([5, 6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(6):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    npv_loop = int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return npv_loop\n",
    "\n",
    "\n",
    "def loop_npv26(gbs, code):\n",
    "    value = np.empty([5, 1])\n",
    "    year = np.array([6, 7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(5):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    npv_loop = int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    return npv_loop\n",
    "\n",
    "\n",
    "def loop_npv27(gbs, code):\n",
    "    value = np.empty([4, 1])\n",
    "    year = np.array([7, 8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(4):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv28(gbs, code):\n",
    "    value = np.empty([3, 1])\n",
    "    year = np.array([8, 9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(3):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv29(gbs, code):\n",
    "    value = np.empty([2, 1])\n",
    "    year = np.array([9, 10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(2):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def loop_npv30(gbs, code):\n",
    "    value = np.empty([1, 1])\n",
    "    year = np.array([10])\n",
    "    array = morph_array_pole(code)\n",
    "    for m in range(1):\n",
    "        value[m] = gbs * 365 * array[m][3] * array[m][2] - array[m][1]\n",
    "    year_0 = 0\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(\n",
    "        sum(\n",
    "            [\n",
    "                value_i / ((1.0 + disc_rt) ** (year_i - year_0))\n",
    "                for value_i, year_i in zip(value, year)\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def rx_calc(path_loss_umi_db):\n",
    "    rx_signal_strength_db = 37.0 - path_loss_umi_db\n",
    "    sinr = (rx_signal_strength_db - rx_sensitivity_db).round(0).astype(int)\n",
    "    rx_signal_strength_mw = 10 ** ((37.0 - path_loss_umi_db) / 10) / 1000.000000000\n",
    "    return rx_signal_strength_db, sinr, rx_signal_strength_mw\n",
    "\n",
    "\n",
    "def sinr_new(sum_rx_signal, rx_signal_strength_db):\n",
    "    # note that -89.4 here is rx_sensitivity_db\n",
    "    sinr_nouveau = np.round(\n",
    "        (\n",
    "            sum_rx_signal.where(\n",
    "                sum_rx_signal.isnull(),\n",
    "                rx_signal_strength_db\n",
    "                - (np.log10((sum_rx_signal + (10 ** (-89.4 / 10)) / 1000) * 1000) * 10),\n",
    "            )\n",
    "        ),\n",
    "        0,\n",
    "    )\n",
    "    return sinr_nouveau\n",
    "\n",
    "\n",
    "# SECTION 3: FIRST ITERATION TO GENERATE INHERENT NPV\n",
    "\n",
    "pole_du = fin_arrays_pole(1)\n",
    "pole_u = fin_arrays_pole(2)\n",
    "pole_s = fin_arrays_pole(3)\n",
    "\n",
    "sites[\"asset_id\"] = sites.apply(lambda x: asset(x[\"Type\"]), axis=1)\n",
    "sites[\"morph_id\"] = sites.apply(lambda x: morph(x[\"Morphology\"]), axis=1)\n",
    "sites[\"morph_code\"] = sites.apply(\n",
    "    lambda x: asset_morph(x[\"asset_id\"], x[\"morph_id\"]), axis=1\n",
    ")\n",
    "sites = sites.drop([\"Type\", \"Morphology\", \"asset_id\", \"morph_id\"], axis=1)\n",
    "site_t_m = sites.groupby(\"fict_site\")[\"morph_code\"].mean().reset_index()\n",
    "init_sites = sites\n",
    "\n",
    "# calculation of build years from inherent npv\n",
    "site_bin_gbs = sites.groupby(\"fict_site\")[\"Hour_GBs\"].sum().reset_index()\n",
    "bld_yr_sites = pd.merge(site_bin_gbs, site_t_m, on=\"fict_site\")\n",
    "\n",
    "bld_yr_sites[\"1\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv21(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"2\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv22(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"3\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv23(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"4\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv24(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"5\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv25(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"6\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv26(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"7\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv27(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"8\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv28(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"9\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv29(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"10\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv30(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "\n",
    "inh_bld_yr = bld_yr_sites[[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]]\n",
    "\n",
    "# chooses min and excludes any negative npv values\n",
    "bld_yr_sites[\"build_yr\"] = inh_bld_yr.idxmin(axis=1).astype(int)\n",
    "bld_yr_sites = bld_yr_sites[bld_yr_sites[\"10\"] != 12]\n",
    "print(bld_yr_sites['build_yr'].value_counts())\n",
    "init_build_yr_sites = bld_yr_sites[[\"fict_site\", \"build_yr\"]]\n",
    "\n",
    "sites_yr1 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 1]\n",
    "sites_yr2 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 2]\n",
    "sites_yr3 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 3]\n",
    "sites_yr4 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 4]\n",
    "sites_yr5 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 5]\n",
    "sites_yr6 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 6]\n",
    "sites_yr7 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 7]\n",
    "sites_yr8 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 8]\n",
    "sites_yr9 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 9]\n",
    "sites_yr10 = bld_yr_sites.loc[bld_yr_sites[\"build_yr\"] == 10]\n",
    "\n",
    "site_number = (\n",
    "        len(sites_yr1)\n",
    "        + len(sites_yr2)\n",
    "        + len(sites_yr3)\n",
    "        + len(sites_yr4)\n",
    "        + len(sites_yr5)\n",
    "        + len(sites_yr6)\n",
    "        + len(sites_yr7)\n",
    "        + len(sites_yr8)\n",
    "        + len(sites_yr9)\n",
    "        + len(sites_yr10)\n",
    ")\n",
    "\n",
    "selected = pd.DataFrame([])\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "(\n",
    "    init_sites[\"rx_signal_strength_db\"],\n",
    "    init_sites[\"sinr\"],\n",
    "    init_sites[\"rx_signal_strength_mw\"],\n",
    ") = rx_calc(init_sites[\"path_loss_umi_db\"])\n",
    "init_sites.loc[init_sites[\"sinr\"] > 50, \"sinr\"] = 50.0\n",
    "calc_sites = init_sites[\n",
    "    [\n",
    "        \"fict_site\",\n",
    "        \"GridName\",\n",
    "        \"Hour_GBs\",\n",
    "        \"sinr\",\n",
    "        \"rx_signal_strength_db\",\n",
    "        \"rx_signal_strength_mw\",\n",
    "        \"morph_code\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# note that declaring variables and following if statement avoid errors\n",
    "sites_yr1 = sites_yr1[[\"fict_site\"]]\n",
    "sites_1 = pd.merge(calc_sites, sites_yr1, on=\"fict_site\")\n",
    "start_yr1 = timeit.default_timer()\n",
    "candidates = pd.DataFrame([])\n",
    "init_ranking = pd.DataFrame([])\n",
    "bad_candidate = 0.0\n",
    "full = 1\n",
    "\n",
    "if len(sites_yr1) > 0:\n",
    "    sites = sites_1\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv21(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_1, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv21(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "start_yr2 = timeit.default_timer()\n",
    "sites_yr2 = sites_yr2[[\"fict_site\"]]\n",
    "sites_2 = pd.merge(calc_sites, sites_yr2, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr2) > 0:\n",
    "    sites = sites_2\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv22(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_2, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv22(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "    elif len(candidates) == 0:\n",
    "        break\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "sites_yr3 = sites_yr3[[\"fict_site\"]]\n",
    "sites_3 = pd.merge(calc_sites, sites_yr3, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr3) > 0:\n",
    "    sites = sites_3\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv23(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_3, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv23(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "    elif len(candidates) == 0:\n",
    "        break\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "sites_yr4 = sites_yr4[[\"fict_site\"]]\n",
    "sites_4 = pd.merge(calc_sites, sites_yr4, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr4) > 0:\n",
    "    sites = sites_4\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv24(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_4, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv24(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "sites_yr5 = sites_yr5[[\"fict_site\"]]\n",
    "sites_5 = pd.merge(calc_sites, sites_yr5, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr5) > 0:\n",
    "    sites = sites_5\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv25(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_5, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv25(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "sites_yr6 = sites_yr6[[\"fict_site\"]]\n",
    "sites_6 = pd.merge(calc_sites, sites_yr6, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr6) > 0:\n",
    "    sites = sites_6\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv26(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_6, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv26(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "start_yr7 = timeit.default_timer()\n",
    "sites_yr7 = sites_yr7[[\"fict_site\"]]\n",
    "sites_7 = pd.merge(calc_sites, sites_yr7, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr7) > 0:\n",
    "    sites = sites_7\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv27(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_7, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv27(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "end_yr7 = timeit.default_timer()\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "start_yr8 = timeit.default_timer()\n",
    "sites_yr8 = sites_yr8[[\"fict_site\"]]\n",
    "sites_8 = pd.merge(calc_sites, sites_yr8, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr8) > 0:\n",
    "    sites = sites_8\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv28(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_8, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv28(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "sites_yr9 = sites_yr9[[\"fict_site\"]]\n",
    "sites_9 = pd.merge(calc_sites, sites_yr9, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr9) > 0:\n",
    "    sites = sites_9\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv29(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_9, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv29(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = pole_du[1:11, :]\n",
    "pole_u = pole_u[1:11, :]\n",
    "pole_s = pole_s[1:11, :]\n",
    "\n",
    "start_yr10 = timeit.default_timer()\n",
    "sites_yr10 = sites_yr10[[\"fict_site\"]]\n",
    "sites_10 = pd.merge(calc_sites, sites_yr10, on=\"fict_site\")\n",
    "\n",
    "if len(sites_yr10) > 0:\n",
    "    sites = sites_10\n",
    "    sites = sites.groupby(\"fict_site\", as_index=False).agg(\n",
    "        {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "    )\n",
    "    sites[\"npv\"] = sites.apply(\n",
    "        lambda x: loop_npv30(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "    ).astype(int)\n",
    "    ranking = sites.loc[sites[\"npv\"] > 0].copy()\n",
    "    ranking[\"rank\"] = ranking[\"npv\"].rank(ascending=False)\n",
    "    ranking = ranking.sort_values(by=\"rank\")\n",
    "    temp_ranking = ranking[[\"fict_site\"]]\n",
    "    init_ranking_bins = pd.merge(temp_ranking, sites_10, on=\"fict_site\")\n",
    "    init_ranking_counts = init_ranking_bins[\"GridName\"].value_counts().reset_index()\n",
    "    init_ranking_counts = init_ranking_counts.rename(\n",
    "        columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    "    )\n",
    "    init_ranking = pd.merge(init_ranking_bins, init_ranking_counts, on=\"GridName\")\n",
    "    if len(selected) > 0 and len(ranking) > 0:\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "        new_selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected.append(new_selected, sort=True)\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "    elif len(selected) == 0 and len(ranking) > 0:\n",
    "        selected = (pd.DataFrame(ranking.iloc[0, :])).T\n",
    "        selected = selected[[\"fict_site\", \"npv\"]]\n",
    "        candidates = pd.DataFrame(ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "        candidates = candidates[[\"fict_site\", \"npv\"]]\n",
    "    else:\n",
    "        candidates = pd.DataFrame([])\n",
    "\n",
    "for j in range(len(candidates)):\n",
    "    temp_nb = pd.DataFrame([])\n",
    "    for i in range(len(candidates)):\n",
    "        next_best = (pd.DataFrame(candidates.iloc[i, :])).T\n",
    "        sinr_sites = selected.append(next_best, sort=True)\n",
    "        sinr_bins = pd.merge(init_ranking, sinr_sites, on=\"fict_site\")\n",
    "        sinr_bins_unique = sinr_bins[sinr_bins[\"bin_count\"] == 1].copy()\n",
    "        sinr_bins_dups = sinr_bins[sinr_bins[\"bin_count\"] > 1].copy()\n",
    "        temp_sinr_bins_dups = (\n",
    "            sinr_bins_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"]\n",
    "                .sum()\n",
    "                .reset_index()\n",
    "        )\n",
    "        sinr_bins_dups = sinr_bins_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "        sinr_bins_dups = pd.merge(sinr_bins_dups, temp_sinr_bins_dups, on=\"GridName\")\n",
    "        sinr_bins_unique[\"sinr_new\"] = sinr_bins_unique[\"sinr\"]\n",
    "        sinr_bins_dups[\"sinr_new\"] = sinr_new(\n",
    "            sinr_bins_dups[\"rx_signal_strength_mw\"],\n",
    "            sinr_bins_dups[\"rx_signal_strength_db\"],\n",
    "        )\n",
    "        sinr_bins = sinr_bins_unique.append(sinr_bins_dups, sort=True)\n",
    "        temp_sinr_bins = sinr_bins\n",
    "        sinr_bins = sinr_bins[sinr_bins.sinr_new >= -7.0]\n",
    "        sinr_bins_agg = sinr_bins.groupby(\"fict_site\", as_index=False).agg(\n",
    "            {\"morph_code\": \"mean\", \"Hour_GBs\": \"sum\"}\n",
    "        )\n",
    "        sinr_bins_agg[\"xnpv\"] = sinr_bins_agg.apply(\n",
    "            lambda x: loop_npv30(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    "        ).astype(int)\n",
    "        next_best[\"adj_npv\"] = sinr_bins_agg[\"xnpv\"].min()\n",
    "        next_best[\"sum_npv\"] = sinr_bins_agg[\"xnpv\"].sum()\n",
    "        next_best = next_best[[\"fict_site\", \"adj_npv\", \"sum_npv\"]]\n",
    "        if next_best[\"adj_npv\"].values <= npv_threshold:\n",
    "            bad_candidate = next_best[\"fict_site\"].values\n",
    "            full = 0\n",
    "            break\n",
    "        init_cand_bins = pd.merge(next_best, init_ranking, on=\"fict_site\")\n",
    "        init_cand_bins = init_cand_bins.nlargest(bin_prox, \"sinr\")\n",
    "        init_cand_bins = init_cand_bins[[\"sinr\", \"fict_site\"]].mean().astype(\"int64\")\n",
    "        cand_bins = pd.merge(next_best, temp_sinr_bins, on=\"fict_site\")\n",
    "        cand_bins = cand_bins.nlargest(bin_prox, \"sinr_new\")\n",
    "        cand_bins = cand_bins[[\"sinr_new\"]].mean()\n",
    "        if (abs(init_cand_bins[\"sinr\"] - cand_bins[\"sinr_new\"])) > sinr_deg:\n",
    "            bad_candidate = init_cand_bins[\"fict_site\"].astype(\"int64\")\n",
    "            full = 0\n",
    "            break\n",
    "        temp_nb = temp_nb.append(next_best, sort=True)\n",
    "        full = 1\n",
    "    if full == 1:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        new_ranking = temp_nb\n",
    "        new_ranking[\"rank\"] = new_ranking[\"sum_npv\"].rank(ascending=True)\n",
    "        new_ranking = new_ranking.sort_values(by=\"rank\").reset_index(drop=\"True\")\n",
    "        temp_selected = new_ranking[new_ranking[\"rank\"] == 1]\n",
    "        temp_selected = temp_selected[[\"fict_site\"]]\n",
    "        selected = selected.append(temp_selected, sort=True)\n",
    "        new_ranking = new_ranking[[\"fict_site\"]]\n",
    "        candidates = pd.DataFrame(new_ranking.iloc[1:, :]).reset_index(drop=True)\n",
    "    elif full == 0:\n",
    "        if len(candidates) == 0:\n",
    "            break\n",
    "        candidates = candidates.to_numpy()\n",
    "        candidates = np.delete(\n",
    "            candidates, np.where(candidates == bad_candidate)[0], axis=0\n",
    "        )\n",
    "        candidates = pd.DataFrame(candidates).reset_index(drop=True)\n",
    "        candidates = candidates.rename(columns={0: \"fict_site\"})\n",
    "\n",
    "pole_du = fin_arrays_pole(1)\n",
    "pole_u = fin_arrays_pole(2)\n",
    "pole_s = fin_arrays_pole(3)\n",
    "\n",
    "selected = pd.merge(selected, init_build_yr_sites, on=\"fict_site\")\n",
    "selected = pd.merge(selected, init_sites, on=\"fict_site\")\n",
    "\n",
    "site_bin_gbs = selected.groupby(\"fict_site\")[\"Hour_GBs\"].sum().reset_index()\n",
    "bld_yr_sites = pd.merge(site_bin_gbs, site_t_m, on=\"fict_site\")\n",
    "\n",
    "bld_yr_sites[\"1\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv21(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"2\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv22(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"3\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv23(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"4\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv24(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"5\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv25(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"6\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv26(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"7\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv27(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"8\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv28(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"9\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv29(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "bld_yr_sites[\"10\"] = bld_yr_sites.apply(\n",
    "    lambda x: bld_npv30(x[\"Hour_GBs\"], x[\"morph_code\"]), axis=1\n",
    ")\n",
    "\n",
    "inh_bld_yr = bld_yr_sites[[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"]]\n",
    "\n",
    "bld_yr_sites[\"design_build_yr\"] = inh_bld_yr.idxmin(axis=1).astype(int)\n",
    "final_build_yr_sites = bld_yr_sites[[\"fict_site\", \"design_build_yr\"]]\n",
    "selected = pd.merge(selected, final_build_yr_sites, on=\"fict_site\")\n",
    "selected[\"opt_build_year\"] = (\n",
    "        (selected[\"design_build_yr\"] + selected[\"build_yr\"]) / 2\n",
    ").round(0)\n",
    "\n",
    "selected = selected.drop([\"build_yr\", \"design_build_yr\"], axis=1)\n",
    "selected_bins = selected[\"GridName\"].value_counts().reset_index()\n",
    "selected_bins = selected_bins.rename(\n",
    "    columns={\"index\": \"GridName\", \"GridName\": \"bin_count\"}\n",
    ")\n",
    "selected = pd.merge(selected, selected_bins, on=\"GridName\")\n",
    "\n",
    "selected_unique = selected[selected[\"bin_count\"] == 1].copy()\n",
    "selected_dups = selected[selected[\"bin_count\"] > 1].copy()\n",
    "temp_selected_dups = (\n",
    "    selected_dups.groupby(\"GridName\")[\"rx_signal_strength_mw\"].sum().reset_index()\n",
    ")\n",
    "selected_dups = selected_dups.drop([\"rx_signal_strength_mw\"], axis=1)\n",
    "selected_dups = pd.merge(selected_dups, temp_selected_dups, on=\"GridName\")\n",
    "selected_unique[\"sinr_new\"] = selected_unique[\"sinr\"]\n",
    "selected_dups[\"sinr_new\"] = sinr_new(\n",
    "    selected_dups[\"rx_signal_strength_mw\"], selected_dups[\"rx_signal_strength_db\"]\n",
    ")\n",
    "selected = selected_unique.append(selected_dups, sort=True)\n",
    "\n",
    "selected = selected[selected.sinr_new >= -7.0]\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "temp_selected = selected.groupby(\"fict_site\")[\"morph_code\"].first().reset_index()\n",
    "print(temp_selected[\"morph_code\"].value_counts())\n",
    "\n",
    "print(\"final_sites:\", end=\" \")\n",
    "print(len(selected[\"fict_site\"].unique()))\n",
    "\n",
    "print(\"site_count:\", end=\" \")\n",
    "print(site_number)\n",
    "\n",
    "print(\"total_sec:\", end=\" \")\n",
    "print(end_time - init_time)\n",
    "\n",
    "print(\"total_min:\", end=\" \")\n",
    "print((end_time - init_time) / 60)\n",
    "\n",
    "print(\"sec/site:\", end=\" \")\n",
    "print((end_time - init_time) / site_number)\n",
    "\n",
    "print(\"mean_sinr\", end=\" \")\n",
    "print(selected[\"sinr_new\"].mean())\n",
    "\n",
    "# NPV DATA\n",
    "\n",
    "npv_values.columns = ['non', 'pole_du', 'pole_u', 'pole_s', '', 'off_du', 'off_u',\n",
    "                      'off_s', 'off_r', 'roe_du', 'roe_u', 'roe_s', 'roe_r',\n",
    "                      'smb_du', 'smb_u', 'smb_s', 'smb_r']\n",
    "\n",
    "# This creates time periods for npv\n",
    "st_date20 = pd.date_range(start='2020-06-30', periods=11, freq='12M')\n",
    "st_date21 = pd.date_range(start='2021-06-30', periods=10, freq='12M')\n",
    "st_date22 = pd.date_range(start='2022-06-30', periods=9, freq='12M')\n",
    "st_date23 = pd.date_range(start='2023-06-30', periods=8, freq='12M')\n",
    "st_date24 = pd.date_range(start='2024-06-30', periods=7, freq='12M')\n",
    "st_date25 = pd.date_range(start='2025-06-30', periods=6, freq='12M')\n",
    "st_date26 = pd.date_range(start='2026-06-30', periods=5, freq='12M')\n",
    "st_date27 = pd.date_range(start='2027-06-30', periods=4, freq='12M')\n",
    "st_date28 = pd.date_range(start='2028-06-30', periods=3, freq='12M')\n",
    "st_date29 = pd.date_range(start='2029-06-30', periods=2, freq='12M')\n",
    "st_date30 = pd.date_range(start='2030-06-30', periods=1, freq='12M')\n",
    "\n",
    "disc_rt = .15\n",
    "\n",
    "# LTE INPUTS\n",
    "\n",
    "rx_sensitivity_db = -89.4\n",
    "\n",
    "\n",
    "# SECTION 2: FUNCTIONS\n",
    "\n",
    "def fin_arrays(code):\n",
    "    global npv\n",
    "    cpx = np.empty([11, 1])\n",
    "    opx = np.empty([11, 1])\n",
    "    growth = np.empty([11, 1])\n",
    "    mvno = np.empty([11, 1])\n",
    "    if code == 1:\n",
    "        npv = npv_values.loc[:, 'pole_du']\n",
    "    elif code == 2:\n",
    "        npv = npv_values.loc[:, 'pole_u']\n",
    "    elif code == 3:\n",
    "        npv = npv_values.loc[:, 'pole_s']\n",
    "    elif code == 4:\n",
    "        npv = npv_values.loc[:, '']\n",
    "    elif code == 5:\n",
    "        npv = npv_values.loc[:, 'off_du']\n",
    "    elif code == 6:\n",
    "        npv = npv_values.loc[:, 'off_u']\n",
    "    elif code == 7:\n",
    "        npv = npv_values.loc[:, 'off_s']\n",
    "    elif code == 8:\n",
    "        npv = npv_values.loc[:, 'off_r']\n",
    "    elif code == 9:\n",
    "        npv = npv_values.loc[:, 'roe_du']\n",
    "    elif code == 10:\n",
    "        npv = npv_values.loc[:, 'roe_u']\n",
    "    elif code == 11:\n",
    "        npv = npv_values.loc[:, 'roe_s']\n",
    "    elif code == 12:\n",
    "        npv = npv_values.loc[:, 'roe_r']\n",
    "    elif code == 13:\n",
    "        npv = npv_values.loc[:, 'smb_du']\n",
    "    elif code == 14:\n",
    "        npv = npv_values.loc[:, 'smb_u']\n",
    "    elif code == 15:\n",
    "        npv = npv_values.loc[:, 'smb_s']\n",
    "    elif code == 16:\n",
    "        npv = npv_values.loc[:, 'smb_r']\n",
    "    for i in range(11):\n",
    "        cpx[i] = float(npv[i + 1])\n",
    "    for i in range(11):\n",
    "        opx[i] = float(npv[i + 12])\n",
    "    for i in range(11):\n",
    "        growth[i] = float(npv[i + 23])\n",
    "    for i in range(11):\n",
    "        mvno[i] = float(npv[i + 34])\n",
    "    array = np.hstack((cpx, opx, growth, mvno))\n",
    "    return array\n",
    "\n",
    "\n",
    "def morph_array(code):\n",
    "    if code == 1:\n",
    "        return pole_du\n",
    "    elif code == 2:\n",
    "        return pole_u\n",
    "    elif code == 3:\n",
    "        return pole_s\n",
    "\n",
    "\n",
    "day_diff = 0.0\n",
    "cell_split = 2.0\n",
    "\n",
    "\n",
    "def capex_21(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[1:11, :]\n",
    "    debit = 0\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_22(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[2:11, :]\n",
    "    for i in range(9):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_23(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[3:11, :]\n",
    "    debit = 0\n",
    "    for i in range(8):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_24(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[4:11, :]\n",
    "    debit = 0\n",
    "    for i in range(7):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_25(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[5:11, :]\n",
    "    debit = 0\n",
    "    for i in range(6):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_26(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[6:11, :]\n",
    "    debit = 0\n",
    "    for i in range(5):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_27(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[7:11, :]\n",
    "    debit = 0\n",
    "    for i in range(4):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_28(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[8:11, :]\n",
    "    debit = 0\n",
    "    for i in range(3):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_29(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[9:11, :]\n",
    "    debit = 0\n",
    "    for i in range(2):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def capex_30(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[10:11, :]\n",
    "    debit = 0\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = array[i][0]\n",
    "            return debit\n",
    "\n",
    "\n",
    "def opex_21(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[1:11, :]\n",
    "    debit = 0\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_22(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[2:11, :]\n",
    "    debit = 0\n",
    "    for i in range(9):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_23(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[3:11, :]\n",
    "    debit = 0\n",
    "    for i in range(8):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_24(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[4:11, :]\n",
    "    debit = 0\n",
    "    for i in range(7):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_25(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[5:11, :]\n",
    "    debit = 0\n",
    "    for i in range(6):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_26(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[6:11, :]\n",
    "    debit = 0\n",
    "    for i in range(5):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_27(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[7:11, :]\n",
    "    debit = 0\n",
    "    for i in range(4):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_28(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[8:11, :]\n",
    "    debit = 0\n",
    "    for i in range(3):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_29(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[9:11, :]\n",
    "    debit = 0\n",
    "    for i in range(2):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def opex_30(gbs, mo_cap, code):\n",
    "    array = morph_array(code)\n",
    "    array = array[10:11, :]\n",
    "    debit = 0\n",
    "    for i in range(1):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            debit = debit + array[i][1]\n",
    "    return debit\n",
    "\n",
    "\n",
    "def split21(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    array = morph_array(code)\n",
    "    array = array[1:11, :]\n",
    "    year = 1\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split22(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([9, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[2:11, :]\n",
    "    year = 2\n",
    "    for i in range(9):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split23(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([8, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[3:11, :]\n",
    "    year = 3\n",
    "    for i in range(8):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split24(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([7, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[4:11, :]\n",
    "    year = 4\n",
    "    for i in range(7):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split25(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([6, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[5:11, :]\n",
    "    year = 5\n",
    "    for i in range(6):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split26(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([5, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[6:11, :]\n",
    "    year = 6\n",
    "    for i in range(5):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split27(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([4, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[7:11, :]\n",
    "    year = 7\n",
    "    for i in range(4):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split28(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([3, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[8:11, :]\n",
    "    year = 8\n",
    "    for i in range(3):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split29(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([2, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[9:11, :]\n",
    "    year = 9\n",
    "    for i in range(2):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def split30(gbs, mo_cap, code):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([1, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[10:11, :]\n",
    "    year = 10\n",
    "    for i in range(1):\n",
    "        if (gbs * (array[i][2]) * 365) >= (mo_cap * 365):\n",
    "            return year\n",
    "        else:\n",
    "            year = year + 1\n",
    "\n",
    "\n",
    "def npv21(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([10, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[1:11, :]\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(sum([value_i / ((1.0 + disc_rt) ** ((date_i - date_0).days / 365.0))\n",
    "                    for value_i, date_i in zip(value, st_date21)]))\n",
    "\n",
    "\n",
    "def npv22(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([9, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[2:11, :]\n",
    "    a = 0\n",
    "    for i in range(9):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date22)]))\n",
    "\n",
    "\n",
    "def npv23(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([8, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[3:11, :]\n",
    "    a = 0\n",
    "    for i in range(8):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date23)]))\n",
    "\n",
    "\n",
    "def npv24(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([7, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[4:11, :]\n",
    "    a = 0\n",
    "    for i in range(7):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date24)]))\n",
    "\n",
    "\n",
    "def npv25(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([6, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[5:11, :]\n",
    "    a = 0\n",
    "    for i in range(6):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date25)]))\n",
    "\n",
    "\n",
    "def npv26(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([5, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[6:11, :]\n",
    "    a = 0\n",
    "    for i in range(5):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date26)]))\n",
    "\n",
    "\n",
    "def npv27(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([4, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[7:11, :]\n",
    "    a = 0\n",
    "    for i in range(4):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date27)]))\n",
    "\n",
    "\n",
    "def npv28(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([3, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[8:11, :]\n",
    "    for i in range(3):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[0] = value[0] - array[0][0]\n",
    "    return int(sum([value_i / ((1.0 + disc_rt) ** ((date_i - date_0).days / 365.0))\n",
    "                    for value_i, date_i in zip(value, st_date28)]))\n",
    "\n",
    "\n",
    "def npv29(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([2, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[9:11, :]\n",
    "    a = 0\n",
    "    for i in range(2):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date29)]))\n",
    "\n",
    "\n",
    "def npv30(gbs, code, mo_cap):\n",
    "    # slice 0 is cpx, 1 is opx, 2 is growth, 3 is mvno,\n",
    "    value = np.empty([1, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[10:11, :]\n",
    "    a = 0\n",
    "    for i in range(1):\n",
    "        if (gbs * (array[i][2]) * 365) < (cell_split * mo_cap * 365):\n",
    "            value[i] = gbs * 365 * array[i][3] * array[i][2] - array[i][1]\n",
    "        else:\n",
    "            value[i] = cell_split * mo_cap * 365\n",
    "    date_0 = st_date20[0]\n",
    "    value[a] = value[a] - array[a][0]\n",
    "    return int(\n",
    "        sum([value_i / ((1.0 + disc_rt) ** (((date_i - date_0).days - day_diff) / 365.0))\n",
    "             for value_i, date_i in zip(value, st_date30)]))\n",
    "\n",
    "\n",
    "def extra_offload21(gbs, code, mo_cap):\n",
    "    cap = np.empty([10, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[1:11, :]\n",
    "    for i in range(10):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload22(gbs, code, mo_cap):\n",
    "    cap = np.empty([9, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[2:11, :]\n",
    "    for i in range(9):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload23(gbs, code, mo_cap):\n",
    "    cap = np.empty([8, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[3:11, :]\n",
    "    for i in range(8):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload24(gbs, code, mo_cap):\n",
    "    cap = np.empty([7, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[4:11, :]\n",
    "    for i in range(7):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload25(gbs, code, mo_cap):\n",
    "    cap = np.empty([6, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[5:11, :]\n",
    "    for i in range(6):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload26(gbs, code, mo_cap):\n",
    "    cap = np.empty([5, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[6:11, :]\n",
    "    for i in range(5):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload27(gbs, code, mo_cap):\n",
    "    cap = np.empty([4, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[7:11, :]\n",
    "    for i in range(4):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload28(gbs, code, mo_cap):\n",
    "    cap = np.empty([3, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[8:11, :]\n",
    "    for i in range(3):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload29(gbs, code, mo_cap):\n",
    "    cap = np.empty([2, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[9:11, :]\n",
    "    for i in range(2):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def extra_offload30(gbs, code, mo_cap):\n",
    "    cap = np.empty([1, 1])\n",
    "    array = morph_array(code)\n",
    "    array = array[10:11, :]\n",
    "    for i in range(1):\n",
    "        if (gbs * (array[i][2]) * 365) < (mo_cap * 365):\n",
    "            cap[i] = (mo_cap * 365) - (gbs * (array[i][2]) * 365)\n",
    "        else:\n",
    "            cap[i] = 0\n",
    "    return cap\n",
    "\n",
    "\n",
    "def rb_thru_put(code_rate, symbols, mimo, subframe, retrans, high_layer_over,\n",
    "                over_tp_kbps):\n",
    "    rb_thru = (((code_rate * symbols * 4800000 * mimo * subframe) / 1000000) *\n",
    "               ((1 - retrans) * (1 - high_layer_over)) - (over_tp_kbps / 1000)) / 400\n",
    "    return rb_thru\n",
    "\n",
    "\n",
    "def rx_calc(path_loss_umi_db):\n",
    "    rx_signal_strength_db = 37.0 - path_loss_umi_db\n",
    "    sinr = (rx_signal_strength_db - rx_sensitivity_db).round(0).astype(int)\n",
    "    rx_signal_strength_mw = (10 ** ((37.0 - path_loss_umi_db) / 10) / 1000.000000000)\n",
    "    return rx_signal_strength_db, sinr, rx_signal_strength_mw\n",
    "\n",
    "\n",
    "def sinr_new(sum_rx_signal, rx_signal_strength_db):\n",
    "    # note that -89.4 here is rx_sensitivity_db\n",
    "    sinr_nouveau = np.round(\n",
    "        sum_rx_signal.where(sum_rx_signal.isnull(), rx_signal_strength_db -\n",
    "                            (np.log10((sum_rx_signal + (\n",
    "                                        10 ** (-89.4 / 10)) / 1000) * 1000) * 10)),\n",
    "        0)\n",
    "    return sinr_nouveau\n",
    "\n",
    "\n",
    "def sinr_new2(sum_rx_signal, rx_signal_strength_db):\n",
    "    # note that -89.4 here is rx_sensitivity_db\n",
    "    sinr_nu = np.round(sum_rx_signal.where(sum_rx_signal.isnull(), rx_signal_strength_db -\n",
    "                                           (np.log10((sum_rx_signal + (10 ** (\n",
    "                                                       -89.4 / 10)) / 1000) * 1000) * 10)),\n",
    "                       0)\n",
    "    return sinr_nu\n",
    "\n",
    "\n",
    "def reverse_morph_type(code):\n",
    "    global type, morph, type_morph\n",
    "    if code == 1:\n",
    "        type = 'Pole'\n",
    "        morph = 'Dense Urban'\n",
    "        type_morph = 'Pole/Dense Urban'\n",
    "    elif code == 2:\n",
    "        type = 'Pole'\n",
    "        morph = 'Urban'\n",
    "        type_morph = 'Pole/Urban'\n",
    "    elif code == 3:\n",
    "        type = 'Pole'\n",
    "        morph = 'Suburban'\n",
    "        type_morph = 'Pole / Suburban'\n",
    "    return type\n",
    "\n",
    "\n",
    "def reverse_morph_morph(code):\n",
    "    global type, morph, type_morph\n",
    "    if code == 1:\n",
    "        type = 'Pole'\n",
    "        morph = 'Dense Urban'\n",
    "        type_morph = 'Pole/Dense Urban'\n",
    "    elif code == 2:\n",
    "        type = 'Pole'\n",
    "        morph = 'Urban'\n",
    "        type_morph = 'Pole/Urban'\n",
    "    elif code == 3:\n",
    "        type = 'Pole'\n",
    "        morph = 'Suburban'\n",
    "        type_morph = 'Pole / Suburban'\n",
    "    return morph\n",
    "\n",
    "\n",
    "def reverse_morph_type_morph(code):\n",
    "    global type, morph, type_morph\n",
    "    if code == 1:\n",
    "        type = 'Pole'\n",
    "        morph = 'Dense Urban'\n",
    "        type_morph = 'Pole/Dense Urban'\n",
    "    elif code == 2:\n",
    "        type = 'Pole'\n",
    "        morph = 'Urban'\n",
    "        type_morph = 'Pole/Urban'\n",
    "    elif code == 3:\n",
    "        type = 'Pole'\n",
    "        morph = 'Suburban'\n",
    "        type_morph = 'Pole / Suburban'\n",
    "    return type_morph\n",
    "\n",
    "\n",
    "pole_du = fin_arrays(1)\n",
    "pole_u = fin_arrays(2)\n",
    "pole_s = fin_arrays(3)\n",
    "off_du = fin_arrays(5)\n",
    "off_u = fin_arrays(6)\n",
    "off_s = fin_arrays(7)\n",
    "off_r = fin_arrays(8)\n",
    "roe_du = fin_arrays(9)\n",
    "roe_u = fin_arrays(10)\n",
    "roe_s = fin_arrays(11)\n",
    "roe_r = fin_arrays(12)\n",
    "smb_du = fin_arrays(13)\n",
    "smb_u = fin_arrays(14)\n",
    "smb_s = fin_arrays(15)\n",
    "smb_r = fin_arrays(16)\n",
    "\n",
    "all_segs_init = selected\n",
    "\n",
    "all_segs = all_segs_init\n",
    "\n",
    "all_segs['rx_signal_strength_db'], all_segs['sinr'], all_segs['rx_signal_strength_mw'] = \\\n",
    "    rx_calc(all_segs['path_loss_umi_db'])\n",
    "all_segs.loc[all_segs['sinr'] > 50, 'sinr'] = 50.0\n",
    "\n",
    "sinr_bins_unique = all_segs.drop_duplicates(subset='GridName', keep=False).copy()\n",
    "sinr_bins_dups = all_segs[all_segs.duplicated(['GridName'], keep=False)].copy()\n",
    "sinr_bins_dups['sum_rx_signal'] = sinr_bins_dups.groupby('GridName')[\n",
    "    'rx_signal_strength_mw'].transform(sum)\n",
    "all_segs = sinr_bins_unique.append(sinr_bins_dups)\n",
    "sinr_bins_a = all_segs[all_segs['sum_rx_signal'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "sinr_bins_a['sinr_new'] = sinr_new2(sinr_bins_a['sum_rx_signal'],\n",
    "                                    sinr_bins_a['rx_signal_strength_db'])\n",
    "\n",
    "sinr_bins_b = all_segs[all_segs['sum_rx_signal'].isnull()].copy()\n",
    "sinr_bins_b['sinr_new'] = sinr_bins_b['sum_rx_signal'].where(\n",
    "    sinr_bins_b['sum_rx_signal'].notnull(),\n",
    "    sinr_bins_b['sinr'], axis=0)\n",
    "all_segs = sinr_bins_a.append(sinr_bins_b)\n",
    "all_segs = all_segs[all_segs.sinr_new >= -2.0]\n",
    "all_segs['gb_offload'] = all_segs.groupby('fict_site')['Hour_GBs'].transform(sum)\n",
    "all_segs = all_segs.drop(['rx_signal_strength_db', 'rx_signal_strength_mw', 'sinr',\n",
    "                          'sum_rx_signal', 'gb_offload'], axis=1)\n",
    "all_segs['grid_temp'] = all_segs['GridName'].str.replace(r'\\D', '')\n",
    "all_segs['grid_temp'] = all_segs['grid_temp'].str[-8:]\n",
    "all_segs = all_segs.sort_values(['grid_temp', 'sinr_new', 'opt_build_year'])\n",
    "\n",
    "all_segs_unique = all_segs.drop_duplicates(subset='grid_temp', keep=False).copy()\n",
    "all_segs_dups = all_segs[all_segs.duplicated(['grid_temp'], keep=False)].copy()\n",
    "all_segs = all_segs_unique.append(all_segs_dups)\n",
    "\n",
    "all_segs = pd.merge(all_segs, lte_params, left_on='sinr_new', right_on='SINR')\n",
    "all_segs['rb_thru_put'] = rb_thru_put(all_segs['Code Rate'], all_segs['symbols/SF'],\n",
    "                                      all_segs['2x2 MIMO Gain'],\n",
    "                                      all_segs['subframe allocation'],\n",
    "                                      all_segs['retrans'],\n",
    "                                      all_segs['high  layer overhead'],\n",
    "                                      all_segs['overhead TP kbps'])\n",
    "all_segs['gb_offload'] = all_segs.groupby('fict_site')['Hour_GBs'].transform(sum)\n",
    "print(lte_params['Code Rate'].mean())\n",
    "all_segs['rx_signal_strength_db'], all_segs['sinr'], all_segs['rx_signal_strength_mw'] = \\\n",
    "    rx_calc(all_segs['path_loss_umi_db'])\n",
    "all_segs.loc[all_segs['sinr'] > 50, 'sinr'] = 50.0\n",
    "\n",
    "fict_site_count_a = all_segs[['GridName', 'fict_site']]\n",
    "\n",
    "fict_site_count = fict_site_count_a['fict_site'].value_counts().reset_index()\n",
    "fict_site_count = fict_site_count.rename(\n",
    "    columns={'index': 'fict_site', 'fict_site': 'fict_site_count'})\n",
    "all_segs = pd.merge(all_segs, fict_site_count, on='fict_site')\n",
    "\n",
    "all_segs = all_segs.loc[all_segs['fict_site_count'] > 61]\n",
    "\n",
    "sinr_bins_unique = all_segs.drop_duplicates(subset='GridName', keep=False).copy()\n",
    "sinr_bins_dups = all_segs[all_segs.duplicated(['GridName'], keep=False)].copy()\n",
    "sinr_bins_dups['sum_rx_signal'] = sinr_bins_dups.groupby('GridName')[\n",
    "    'rx_signal_strength_mw'].transform(sum)\n",
    "all_segs = sinr_bins_unique.append(sinr_bins_dups)\n",
    "sinr_bins_a = all_segs[all_segs['sum_rx_signal'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "sinr_bins_a['sinr_new'] = sinr_new2(sinr_bins_a['sum_rx_signal'],\n",
    "                                    sinr_bins_a['rx_signal_strength_db'])\n",
    "\n",
    "sinr_bins_b = all_segs[all_segs['sum_rx_signal'].isnull()].copy()\n",
    "sinr_bins_b['sinr_new'] = sinr_bins_b['sum_rx_signal'].where(\n",
    "    sinr_bins_b['sum_rx_signal'].notnull(),\n",
    "    sinr_bins_b['sinr'], axis=0)\n",
    "all_segs = sinr_bins_a.append(sinr_bins_b)\n",
    "all_segs = all_segs[all_segs.sinr_new >= -2.0]\n",
    "all_segs['gb_offload'] = all_segs.groupby('fict_site')['Hour_GBs'].transform(sum)\n",
    "all_segs = all_segs.drop(['path_loss_umi_db', 'Code Rate', 'symbols/SF', '2x2 MIMO Gain',\n",
    "                          'rx_signal_strength_db', 'rx_signal_strength_mw', 'sinr',\n",
    "                          'sum_rx_signal', 'gb_offload', 'subframe allocation',\n",
    "                          'retrans', 'high  layer overhead', 'overhead TP kbps'], axis=1)\n",
    "\n",
    "all_segs['grid_temp'] = all_segs['GridName'].str.replace(r'\\D', '')\n",
    "all_segs['grid_temp'] = all_segs['grid_temp'].str[-8:]\n",
    "all_segs = all_segs.sort_values(['grid_temp', 'sinr_new', 'opt_build_year'])\n",
    "\n",
    "all_segs_unique = all_segs.drop_duplicates(subset='grid_temp', keep=False).copy()\n",
    "all_segs_dups = all_segs[all_segs.duplicated(['grid_temp'], keep=False)].copy()\n",
    "all_segs = all_segs_unique.append(all_segs_dups)\n",
    "\n",
    "all_segs = pd.merge(all_segs, lte_params, left_on='sinr_new', right_on='SINR')\n",
    "all_segs['rb_thru_put'] = rb_thru_put(all_segs['Code Rate'], all_segs['symbols/SF'],\n",
    "                                      all_segs['2x2 MIMO Gain'],\n",
    "                                      all_segs['subframe allocation'],\n",
    "                                      all_segs['retrans'],\n",
    "                                      all_segs['high  layer overhead'],\n",
    "                                      all_segs['overhead TP kbps'])\n",
    "all_segs['gb_offload'] = all_segs.groupby('fict_site')['Hour_GBs'].transform(sum)\n",
    "test = all_segs[['GridName', 'fict_site']]\n",
    "\n",
    "fict_site_count_2 = test['fict_site'].value_counts().reset_index()\n",
    "fict_site_count_2 = fict_site_count_2.rename(\n",
    "    columns={'index': 'fict_site', 'fict_site': 'fict_site_count_2'})\n",
    "fict_site_check = pd.merge(fict_site_count, fict_site_count_2, on='fict_site')\n",
    "fict_site_check['diff'] = fict_site_check['fict_site_count'] - fict_site_check[\n",
    "    'fict_site_count_2']\n",
    "\n",
    "all_segs = pd.merge(all_segs, fict_site_count_2, on='fict_site')\n",
    "\n",
    "all_segs = all_segs.sort_values(by=['fict_site'], ascending=True)\n",
    "\n",
    "all_segs = pd.merge(all_segs, bin_req, on='GridName')\n",
    "\n",
    "sinr_test = all_segs\n",
    "all_segs['bh_req_rbs'] = all_segs['bin_req_hr_mbps'] / all_segs['rb_thru_put']\n",
    "all_segs = all_segs[\n",
    "    ['fict_site', 'morph_code', 'gb_offload', 'bh_req_rbs', 'opt_build_year']]\n",
    "all_segs = all_segs.groupby('fict_site', as_index=False).agg(\n",
    "    {'bh_req_rbs': sum, 'morph_code': 'mean', 'gb_offload': 'mean',\n",
    "     'opt_build_year': 'mean'})\n",
    "all_segs['enb_util'] = np.ceil(all_segs['bh_req_rbs']) / 400\n",
    "all_segs['max_cap'] = (all_segs['gb_offload'] / all_segs['enb_util'])\n",
    "all_segs = pd.merge(all_segs, fict_site_count_2, on='fict_site')\n",
    "\n",
    "npv2021 = all_segs[all_segs['opt_build_year'] == 1].copy().reset_index()\n",
    "npv2022 = all_segs[all_segs['opt_build_year'] == 2].copy().reset_index()\n",
    "npv2023 = all_segs[all_segs['opt_build_year'] == 3].copy().reset_index()\n",
    "npv2024 = all_segs[all_segs['opt_build_year'] == 4].copy().reset_index()\n",
    "npv2025 = all_segs[all_segs['opt_build_year'] == 5].copy().reset_index()\n",
    "npv2026 = all_segs[all_segs['opt_build_year'] == 6].copy().reset_index()\n",
    "npv2027 = all_segs[all_segs['opt_build_year'] == 7].copy().reset_index()\n",
    "npv2028 = all_segs[all_segs['opt_build_year'] == 8].copy().reset_index()\n",
    "npv2029 = all_segs[all_segs['opt_build_year'] == 9].copy().reset_index()\n",
    "npv2030 = all_segs[all_segs['opt_build_year'] == 10].copy().reset_index()\n",
    "\n",
    "if len(npv2021) > 0:\n",
    "    npv2021['npv'] = npv2021.apply(\n",
    "        lambda x: npv21(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2021['split_yr'] = npv2021.apply(\n",
    "        lambda x: split21(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2021['debit'] = npv2021.apply(\n",
    "        lambda x: capex_21(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2021.apply(lambda x: opex_21(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2021['npv'] = npv2021['npv'] - npv2021['debit']\n",
    "    extra_capacity_21 = npv2021.apply(\n",
    "        lambda x: extra_offload21(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_21 = np.hstack(extra_capacity_21).T\n",
    "    npv2021 = pd.concat([npv2021, temp_21], axis=1)\n",
    "\n",
    "if len(npv2022) > 0:\n",
    "    npv2022['npv'] = npv2022.apply(\n",
    "        lambda x: npv22(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2022['split_yr'] = npv2022.apply(\n",
    "        lambda x: split22(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2022['debit'] = npv2022.apply(\n",
    "        lambda x: capex_22(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2022.apply(lambda x: opex_22(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2022['npv'] = npv2022['npv'] - npv2022['debit']\n",
    "    extra_capacity_22 = npv2022.apply(\n",
    "        lambda x: extra_offload22(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_22 = np.hstack(extra_capacity_22).T\n",
    "    np_temp_22 = np.zeros([len(npv2022), 1])\n",
    "    temp_22 = pd.DataFrame(np.concatenate((np_temp_22, temp_22), axis=1))\n",
    "    npv2022 = pd.concat([npv2022, temp_22], axis=1)\n",
    "\n",
    "if len(npv2023) > 0:\n",
    "    npv2023['npv'] = npv2023.apply(\n",
    "        lambda x: npv23(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2023['split_yr'] = npv2023.apply(\n",
    "        lambda x: split23(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2023['debit'] = npv2023.apply(\n",
    "        lambda x: capex_23(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2023.apply(lambda x: opex_23(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2023['npv'] = npv2023['npv'] - npv2023['debit']\n",
    "    extra_capacity_23 = npv2023.apply(\n",
    "        lambda x: extra_offload23(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_23 = np.hstack(extra_capacity_23).T\n",
    "    np_temp_23 = np.zeros([len(npv2023), 2])\n",
    "    temp_23 = pd.DataFrame(np.concatenate((np_temp_23, temp_23), axis=1))\n",
    "    npv2023 = pd.concat([npv2023, temp_23], axis=1)\n",
    "\n",
    "if len(npv2024) > 0:\n",
    "    npv2024['npv'] = npv2024.apply(\n",
    "        lambda x: npv24(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2024['split_yr'] = npv2024.apply(\n",
    "        lambda x: split24(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2024['debit'] = npv2024.apply(\n",
    "        lambda x: capex_24(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2024.apply(lambda x: opex_24(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2024['npv'] = npv2024['npv'] - npv2024['debit']\n",
    "    extra_capacity_24 = npv2024.apply(\n",
    "        lambda x: extra_offload24(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_24 = np.hstack(extra_capacity_24).T\n",
    "    np_temp_24 = np.zeros([len(npv2024), 3])\n",
    "    temp_24 = pd.DataFrame(np.concatenate((np_temp_24, temp_24), axis=1))\n",
    "    npv2024 = pd.concat([npv2024, temp_24], axis=1)\n",
    "\n",
    "if len(npv2025) > 0:\n",
    "    npv2025['npv'] = npv2025.apply(\n",
    "        lambda x: npv25(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2025['split_yr'] = npv2025.apply(\n",
    "        lambda x: split25(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2025['debit'] = npv2025.apply(\n",
    "        lambda x: capex_25(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2025.apply(lambda x: opex_25(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2025['npv'] = npv2025['npv'] - npv2025['debit']\n",
    "    extra_capacity_25 = npv2025.apply(\n",
    "        lambda x: extra_offload25(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_25 = np.hstack(extra_capacity_25).T\n",
    "    np_temp_25 = np.zeros([len(npv2025), 4])\n",
    "    temp_25 = pd.DataFrame(np.concatenate((np_temp_25, temp_25), axis=1))\n",
    "    npv2025 = pd.concat([npv2025, temp_25], axis=1)\n",
    "\n",
    "if len(npv2026) > 0:\n",
    "    npv2026['npv'] = npv2026.apply(\n",
    "        lambda x: npv26(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2026['split_yr'] = npv2026.apply(\n",
    "        lambda x: split26(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2026['debit'] = npv2026.apply(\n",
    "        lambda x: capex_26(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2026.apply(lambda x: opex_26(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2026['npv'] = npv2026['npv'] - npv2026['debit']\n",
    "    extra_capacity_26 = npv2026.apply(\n",
    "        lambda x: extra_offload26(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_26 = np.hstack(extra_capacity_26).T\n",
    "    np_temp_26 = np.zeros([len(npv2026), 5])\n",
    "    temp_26 = pd.DataFrame(np.concatenate((np_temp_26, temp_26), axis=1))\n",
    "    npv2026 = pd.concat([npv2026, temp_26], axis=1)\n",
    "\n",
    "if len(npv2027) > 0:\n",
    "    npv2027['npv'] = npv2027.apply(\n",
    "        lambda x: npv27(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2027['split_yr'] = npv2027.apply(\n",
    "        lambda x: split27(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2027['debit'] = npv2027.apply(\n",
    "        lambda x: capex_27(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2027.apply(lambda x: opex_27(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2027['npv'] = npv2027['npv'] - npv2027['debit']\n",
    "    extra_capacity_27 = npv2027.apply(\n",
    "        lambda x: extra_offload27(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_27 = np.hstack(extra_capacity_27).T\n",
    "    np_temp_27 = np.zeros([len(npv2027), 6])\n",
    "    temp_27 = pd.DataFrame(np.concatenate((np_temp_27, temp_27), axis=1))\n",
    "    npv2027 = pd.concat([npv2027, temp_27], axis=1)\n",
    "\n",
    "if len(npv2028) > 0:\n",
    "    npv2028['npv'] = npv2028.apply(\n",
    "        lambda x: npv28(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2028['split_yr'] = npv2028.apply(\n",
    "        lambda x: split28(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2028['debit'] = npv2028.apply(\n",
    "        lambda x: capex_28(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2028.apply(lambda x: opex_28(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2028['npv'] = npv2028['npv'] - npv2028['debit']\n",
    "    extra_capacity_28 = npv2028.apply(\n",
    "        lambda x: extra_offload28(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_28 = np.hstack(extra_capacity_28).T\n",
    "    np_temp_28 = np.zeros([len(npv2028), 7])\n",
    "    temp_28 = pd.DataFrame(np.concatenate((np_temp_28, temp_28), axis=1))\n",
    "    npv2028 = pd.concat([npv2028, temp_28], axis=1)\n",
    "\n",
    "if len(npv2029) > 0:\n",
    "    npv2029['npv'] = npv2029.apply(\n",
    "        lambda x: npv29(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2029['split_yr'] = npv2029.apply(\n",
    "        lambda x: split29(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2029['debit'] = npv2029.apply(\n",
    "        lambda x: capex_29(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2029.apply(lambda x: opex_29(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2029['npv'] = npv2029['npv'] - npv2029['debit']\n",
    "    extra_capacity_29 = npv2029.apply(\n",
    "        lambda x: extra_offload29(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_29 = np.hstack(extra_capacity_29).T\n",
    "    np_temp_29 = np.zeros([len(npv2029), 8])\n",
    "    temp_29 = pd.DataFrame(np.concatenate((np_temp_29, temp_29), axis=1))\n",
    "    npv2029 = pd.concat([npv2029, temp_29], axis=1)\n",
    "\n",
    "if len(npv2030) > 0:\n",
    "    npv2030['npv'] = npv2030.apply(\n",
    "        lambda x: npv30(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).astype(int)\n",
    "    npv2030['split_yr'] = npv2030.apply(\n",
    "        lambda x: split30(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1)\n",
    "    npv2030['debit'] = npv2030.apply(\n",
    "        lambda x: capex_30(x['gb_offload'], x['max_cap'], x['morph_code']), axis=1) + \\\n",
    "                       npv2030.apply(lambda x: opex_30(x['gb_offload'], x['max_cap'],\n",
    "                                                       x['morph_code']), axis=1)\n",
    "    npv2030['npv'] = npv2030['npv'] - npv2030['debit']\n",
    "    extra_capacity_30 = npv2030.apply(\n",
    "        lambda x: extra_offload30(x['gb_offload'], x['morph_code'], x['max_cap']),\n",
    "        axis=1).values.tolist()\n",
    "    temp_30 = np.hstack(extra_capacity_30).T\n",
    "    np_temp_30 = np.zeros([len(npv2030), 9])\n",
    "    temp_30 = pd.DataFrame(np.concatenate((np_temp_30, temp_30), axis=1))\n",
    "    npv2030 = pd.concat([npv2030, temp_30], axis=1)\n",
    "\n",
    "final = pd.concat(\n",
    "    [npv2024, npv2025, npv2026, npv2027, npv2028, npv2029, npv2030, npv2021, npv2022,\n",
    "     npv2023], sort=True, ignore_index=True)\n",
    "# final = final.rename(columns={0: 'year_1_excess', 1: 'year_2_excess', 2: 'year_3_excess', 3: 'year_4_excess',\n",
    "#                               4: 'year_5_excess', 5: 'year_6_excess', 6: 'year_7_excess', 7: 'year_8_excess',\n",
    "#                               8: 'year_9_excess', 9: 'year_10_excess'})\n",
    "\n",
    "# final = final[['fict_site', 'npv', 'fict_site_count', 'opt_build_year', 'gb_offload', 'max_cap', 'morph_code', 'debit',\n",
    "#                'year_1_excess', 'year_2_excess', 'year_3_excess', 'year_4_excess', 'year_5_excess', 'year_6_excess',\n",
    "#                'year_7_excess', 'year_8_excess', 'year_9_excess', 'year_10_excess']]\n",
    "\n",
    "# final = final[['fict_site', 'npv', 'fict_site_count', 'opt_build_year', 'gb_offload', 'max_cap', 'morph_code', 'debit']]\n",
    "\n",
    "final = final.sort_values(by=['fict_site'], ascending=True)\n",
    "final = final.loc[final['npv'] > 0]\n",
    "\n",
    "temp_final_npv = final[['npv']]\n",
    "quant05 = temp_final_npv.quantile(.05).iloc[0]\n",
    "quant10 = temp_final_npv.quantile(.1).iloc[0]\n",
    "quant15 = temp_final_npv.quantile(.15).iloc[0]\n",
    "quant20 = temp_final_npv.quantile(.2).iloc[0]\n",
    "quant25 = temp_final_npv.quantile(.25).iloc[0]\n",
    "quant30 = temp_final_npv.quantile(.3).iloc[0]\n",
    "quant35 = temp_final_npv.quantile(.35).iloc[0]\n",
    "\n",
    "print('final', end=' ')\n",
    "print(final['npv'].sum())\n",
    "print(len(final))\n",
    "\n",
    "final_05 = final.loc[final['npv'] > quant05]\n",
    "print('quant05', end=' ')\n",
    "print(final_05['npv'].sum())\n",
    "print(len(final_05))\n",
    "print(quant05)\n",
    "\n",
    "final_10 = final.loc[final['npv'] > quant10]\n",
    "print('quant10', end=' ')\n",
    "print(final_10['npv'].sum())\n",
    "print(len(final_10))\n",
    "print(quant10)\n",
    "\n",
    "final_15 = final.loc[final['npv'] > quant15]\n",
    "print('quant15', end=' ')\n",
    "print(final_15['npv'].sum())\n",
    "print(len(final_15))\n",
    "print(quant15)\n",
    "\n",
    "final_20 = final.loc[final['npv'] > quant20]\n",
    "print('quant20', end=' ')\n",
    "print(final_20['npv'].sum())\n",
    "print(len(final_20))\n",
    "print(quant20)\n",
    "\n",
    "final_25 = final.loc[final['npv'] > quant25]\n",
    "print('quant25', end=' ')\n",
    "print(final_25['npv'].sum())\n",
    "print(len(final_25))\n",
    "print(quant25)\n",
    "\n",
    "final_30 = final.loc[final['npv'] > quant30]\n",
    "print('quant30', end=' ')\n",
    "print(final_30['npv'].sum())\n",
    "print(len(final_30))\n",
    "print(quant30)\n",
    "\n",
    "final_35 = final.loc[final['npv'] > quant35]\n",
    "print('quant35', end=' ')\n",
    "print(final_35['npv'].sum())\n",
    "print(len(final_35))\n",
    "print(quant35)\n",
    "\n",
    "final['type'] = final.apply(lambda x: reverse_morph_type(x['morph_code']), axis=1)\n",
    "final['morph'] = final.apply(lambda x: reverse_morph_morph(x['morph_code']), axis=1)\n",
    "final['type_morph'] = final.apply(lambda x: reverse_morph_type_morph(x['morph_code']),\n",
    "                                  axis=1)\n",
    "\n",
    "sinr_test = sinr_test[['GridName']]\n",
    "init_sites = pd.merge(sinr_test, init_sites, on='GridName')\n",
    "final = pd.merge(final, init_sites, on='fict_site')\n",
    "\n",
    "final['rx_signal_strength_db'], final['sinr'], final['rx_signal_strength_mw'] = \\\n",
    "    rx_calc(final['path_loss_umi_db'])\n",
    "sinr_bins_unique = final.drop_duplicates(subset='GridName', keep=False).copy()\n",
    "sinr_bins_dups = final[final.duplicated(['GridName'], keep=False)].copy()\n",
    "sinr_bins_dups['sum_rx_signal'] = sinr_bins_dups.groupby('GridName')[\n",
    "    'rx_signal_strength_mw'].transform(sum)\n",
    "final = sinr_bins_unique.append(sinr_bins_dups)\n",
    "sinr_bins_a = final[final['sum_rx_signal'].notnull()].copy().reset_index(drop=True)\n",
    "\n",
    "sinr_bins_a['sinr_new'] = sinr_new2(sinr_bins_a['sum_rx_signal'],\n",
    "                                    sinr_bins_a['rx_signal_strength_db'])\n",
    "\n",
    "sinr_bins_b = final[final['sum_rx_signal'].isnull()].copy()\n",
    "sinr_bins_b['sinr_new'] = sinr_bins_b['sum_rx_signal'].where(\n",
    "    sinr_bins_b['sum_rx_signal'].notnull(),\n",
    "    sinr_bins_b['sinr'], axis=0)\n",
    "final = sinr_bins_a.append(sinr_bins_b)\n",
    "final_bins = final[['GridName', 'sinr_new']]\n",
    "final_bins_sinr = final_bins.groupby('GridName')['sinr_new'].mean().reset_index()\n",
    "final_bins_sinr['sinr'] = round(final_bins_sinr['sinr_new'])\n",
    "print('final_bins_sinr avg_sinr', end='  ')\n",
    "print(round(final_bins_sinr['sinr'].mean()))\n",
    "final_bins_sinr.to_csv('bins.csv')\n",
    "\n",
    "print(timeit.default_timer() - init_time_2)\n",
    "\n",
    "# end of program\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}